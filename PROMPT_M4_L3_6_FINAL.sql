-- ==========================================
-- PROMPT ENGINEERING MODULE 4 - LESSONS 3-6 (FINAL - COMPLETE MODULE 4)
-- Professional Mastery & Ethics
-- ~1200 words each, compact HTML
-- ==========================================

-- LESSON 3: Recursive Prompts (ID: 1ee9a8e9-db56-4ccf-a935-72ff12859f35)
UPDATE lessons SET content = '<div class="lesson-content"><h2>Recursive Prompts</h2><h3>Self-Referential Problem Solving</h3><p>Recursive prompts use AI to improve itself, solve problems by breaking them down, or generate solutions that build on previous solutions. This advanced technique enables sophisticated problem-solving. This lesson teaches you how to use recursion in prompting.</p><h3>What Are Recursive Prompts?</h3><p>Recursive prompts are prompts that reference their own outputs or process. AI generates output, that output becomes input for next iteration, process repeats until base case reached. Similar to recursive functions in programming.</p><h3>Types of Recursion in Prompting</h3><p>Type 1 - Self-Improvement: AI improves its own prompts or outputs. Type 2 - Problem Decomposition: Break problem into smaller sub-problems recursively. Type 3 - Iterative Refinement: Each iteration builds on previous. Type 4 - Tree Search: Explore solution space recursively. Each type serves different purposes.</p><h3>Self-Improving Prompts</h3><p>AI improves its own prompts. Process: Start with initial prompt, AI evaluates and suggests improvements, apply improvements to create better prompt, repeat until optimal. Meta-prompting for prompt optimization.</p><h3>Recursive Problem Decomposition</h3><p>Break complex problems down recursively. Pattern: If problem is simple, solve directly (base case). If problem is complex, break into sub-problems, solve each sub-problem recursively, combine solutions. Classic divide-and-conquer approach.</p><h3>Base Cases</h3><p>Define when to stop recursion. Base cases: Problem is simple enough to solve directly, maximum depth reached, solution quality threshold met, time or cost limit hit. Without base cases, recursion never ends.</p><h3>Recursive Refinement</h3><p>Each iteration refines previous output. Iteration 1: Initial solution. Iteration 2: Refine based on Iteration 1. Iteration 3: Refine based on Iteration 2. Continue until satisfactory. Progressive improvement through recursion.</p><h3>Tree Search with Prompts</h3><p>Explore solution space like a tree. Generate multiple options at each level, evaluate each option, recursively explore promising branches, prune unpromising paths, find optimal solution. Systematic exploration of possibilities.</p><h3>Implementing Recursive Prompts</h3><p>Implementation approaches. Manual: Run each iteration manually. Scripted: Automate with code. Prompt chaining: Chain prompts recursively. API orchestration: Programmatic recursive calls. Choose based on complexity.</p><h3>Recursion Depth Control</h3><p>Prevent infinite recursion. Set: Maximum recursion depth, iteration limits, time limits, cost limits. Monitor depth and stop when limit reached. Safety mechanism essential.</p><h3>Example: Recursive Code Review</h3><p>Iteration 1: Review code, identify issues. Iteration 2: Fix issues, review again. Iteration 3: Fix remaining issues, review again. Continue until no issues found or max iterations. Recursive quality improvement.</p><h3>Example: Recursive Planning</h3><p>Level 1: High-level plan. Level 2: Break each step into sub-steps. Level 3: Break sub-steps into tasks. Level 4: Break tasks into actions. Recursive decomposition of planning.</p><h3>Combining Recursion with Other Techniques</h3><p>Recursion plus chain-of-thought: Show reasoning at each level. Recursion plus few-shot: Examples at each recursion level. Recursion plus validation: Validate at each step. Powerful combinations.</p><h3>Common Recursive Mistakes</h3><p>Mistake 1: No base case (infinite loop). Solution: Always define stopping conditions. Mistake 2: Too deep recursion (excessive cost). Solution: Limit depth. Mistake 3: Losing context (information loss). Solution: Preserve necessary context. Mistake 4: Inefficient recursion (redundant work). Solution: Cache results.</p><h3>Optimization Techniques</h3><p>Make recursion efficient. Memoization: Cache results to avoid recomputation. Tail recursion: Optimize last recursive call. Pruning: Skip unpromising branches. Parallel recursion: Explore branches concurrently. Balance thoroughness and efficiency.</p><h3>Key Takeaways</h3><ul><li>Recursive prompts reference their own outputs or process</li><li>Types: self-improvement, problem decomposition, iterative refinement, tree search</li><li>Always define base cases to prevent infinite recursion</li><li>Control recursion depth with limits</li><li>Use for complex problem-solving and optimization</li><li>Combine with other techniques for power</li><li>Optimize with caching and pruning</li></ul><p>In the next lesson, we will explore Industry Applications and Case Studies.</p></div>'
WHERE id = '1ee9a8e9-db56-4ccf-a935-72ff12859f35';

-- LESSON 4: Industry Applications & Case Studies (ID: 3318d5e9-03c1-4ab0-a1b8-87d3a7e3efca)
UPDATE lessons SET content = '<div class="lesson-content"><h2>Industry Applications and Case Studies</h2><h3>Real-World Prompt Engineering</h3><p>Prompt engineering is transforming industries. This lesson explores real-world applications and case studies across different sectors, showing how organizations use prompt engineering to solve business problems.</p><h3>Healthcare Applications</h3><p>Use cases: Clinical documentation (summarize patient notes), medical research (literature review and synthesis), patient communication (explain conditions in plain language), diagnosis support (analyze symptoms and suggest tests), treatment planning (recommend evidence-based treatments). Considerations: Accuracy is critical, regulatory compliance required, privacy protection essential, human oversight mandatory.</p><h3>Finance and Banking</h3><p>Use cases: Financial analysis (analyze reports and trends), risk assessment (evaluate credit and market risk), fraud detection (identify suspicious patterns), customer service (answer account questions), regulatory compliance (check for compliance issues). Considerations: Security is paramount, accuracy affects decisions, regulatory requirements strict, audit trails necessary.</p><h3>Legal Services</h3><p>Use cases: Contract analysis (review and summarize contracts), legal research (find relevant cases and statutes), document drafting (create legal documents), due diligence (review documents for M&A), compliance monitoring (track regulatory changes). Considerations: Accuracy is critical, confidentiality required, human review mandatory, liability concerns significant.</p><h3>Education and Training</h3><p>Use cases: Personalized tutoring (adapt to student level), content creation (generate educational materials), assessment (create and grade assignments), feedback (provide detailed feedback), curriculum planning (design learning paths). Considerations: Pedagogical soundness important, accessibility required, bias mitigation critical, human oversight valuable.</p><h3>Marketing and Sales</h3><p>Use cases: Content creation (blog posts, social media, ads), customer segmentation (analyze and categorize customers), campaign optimization (test and refine messaging), lead qualification (score and prioritize leads), competitive analysis (analyze competitor strategies). Considerations: Brand consistency important, creativity valued, performance measurable, rapid iteration possible.</p><h3>Software Development</h3><p>Use cases: Code generation (write code from specifications), code review (identify bugs and improvements), documentation (generate technical docs), testing (create test cases), debugging (diagnose and fix issues). Considerations: Code quality critical, security important, testing essential, human review valuable.</p><h3>Customer Support</h3><p>Use cases: Automated responses (answer common questions), ticket routing (categorize and route tickets), sentiment analysis (detect customer emotions), knowledge base (search and retrieve information), escalation (identify when to escalate). Considerations: Accuracy affects satisfaction, tone matters, privacy important, human handoff necessary.</p><h3>Human Resources</h3><p>Use cases: Resume screening (evaluate candidates), job descriptions (create compelling JDs), interview questions (generate relevant questions), onboarding (create training materials), performance reviews (draft review templates). Considerations: Bias mitigation critical, fairness essential, privacy required, human judgment important.</p><h3>Research and Development</h3><p>Use cases: Literature review (summarize research papers), hypothesis generation (suggest research directions), experiment design (plan experiments), data analysis (interpret results), grant writing (draft proposals). Considerations: Accuracy critical, citations important, novelty valued, peer review necessary.</p><h3>Content and Media</h3><p>Use cases: Article writing (generate content), editing (improve and polish), translation (translate content), summarization (create summaries), SEO optimization (optimize for search). Considerations: Quality and originality important, fact-checking necessary, plagiarism concerns, human editing valuable.</p><h3>Case Study: Healthcare Documentation</h3><p>Challenge: Doctors spend hours on documentation. Solution: AI-assisted clinical note generation. Approach: Record patient visit, AI generates structured note, doctor reviews and approves. Results: 50% time savings, improved documentation quality, more time with patients. Key success factors: Accurate medical terminology, HIPAA compliance, doctor oversight, continuous improvement.</p><h3>Case Study: Financial Analysis</h3><p>Challenge: Analysts spend days on quarterly reports. Solution: Automated financial report analysis. Approach: AI analyzes reports, extracts key metrics, identifies trends, generates summary. Results: Analysis time reduced from days to hours, consistent quality, early trend detection. Key success factors: Accurate data extraction, financial domain knowledge, human validation, clear output format.</p><h3>Case Study: Customer Support</h3><p>Challenge: High volume of repetitive support tickets. Solution: AI-powered response suggestions. Approach: AI analyzes ticket, suggests response, agent reviews and sends. Results: 40% faster response times, consistent quality, agent satisfaction improved. Key success factors: Accurate intent detection, appropriate tone, easy agent override, continuous learning.</p><h3>Implementation Patterns</h3><p>Pattern 1 - Augmentation: AI assists humans, not replaces. Pattern 2 - Automation: AI handles routine tasks fully. Pattern 3 - Hybrid: Mix of automation and augmentation. Pattern 4 - Advisory: AI provides recommendations, humans decide. Choose pattern based on risk and complexity.</p><h3>Success Factors</h3><p>Common success factors across industries: Clear use case and goals, appropriate human oversight, continuous monitoring and improvement, user training and adoption, integration with existing workflows, measurement of results. Learn from successful implementations.</p><h3>Key Takeaways</h3><ul><li>Prompt engineering transforms industries from healthcare to finance to education</li><li>Each industry has unique use cases and considerations</li><li>Success requires accuracy, compliance, privacy, and human oversight</li><li>Implementation patterns: augmentation, automation, hybrid, advisory</li><li>Case studies show significant time savings and quality improvements</li><li>Common success factors across industries</li><li>Continuous improvement and measurement essential</li></ul><p>In the next lesson, we will explore Temperature and Creativity Control.</p></div>'
WHERE id = '3318d5e9-03c1-4ab0-a1b8-87d3a7e3efca';

-- LESSON 5: Temperature and Creativity Control (ID: 8f0ae954-d667-4c8a-b269-7d6dd78057ec)
UPDATE lessons SET content = '<div class="lesson-content"><h2>Temperature and Creativity Control</h2><h3>Fine-Tuning AI Outputs</h3><p>Temperature is a key parameter that controls randomness and creativity in AI outputs. Understanding and using temperature effectively is crucial for getting the right balance between consistency and creativity. This lesson teaches you how to master temperature control.</p><h3>What Is Temperature?</h3><p>Temperature controls randomness in AI text generation. Low temperature (0-0.3): Deterministic, consistent, focused. Medium temperature (0.4-0.7): Balanced, moderate variation. High temperature (0.8-2.0): Creative, diverse, unpredictable. Temperature affects which words AI chooses.</p><h3>How Temperature Works</h3><p>AI predicts probability for each possible next word. Temperature adjusts these probabilities. Low temperature: Amplifies differences, favors most likely words. High temperature: Flattens differences, allows less likely words. Controls exploration vs exploitation tradeoff.</p><h3>Temperature 0: Maximum Determinism</h3><p>Always chooses most likely next word. Characteristics: Highly consistent, predictable outputs, minimal creativity, same input produces same output. Use for: Factual tasks, data extraction, structured outputs, consistency critical. Example: Extracting data from documents, generating SQL queries.</p><h3>Temperature 0.3: Low Creativity</h3><p>Mostly consistent with slight variation. Characteristics: Very consistent, minimal randomness, focused outputs, reliable quality. Use for: Professional writing, technical documentation, customer support, quality important. Example: Writing business emails, technical explanations.</p><h3>Temperature 0.7: Balanced</h3><p>Good balance of consistency and creativity. Characteristics: Moderate variation, natural language, good quality, reasonable diversity. Use for: General content, conversational AI, most tasks, default choice. Example: Blog posts, chatbots, general writing.</p><h3>Temperature 1.0: High Creativity</h3><p>Significant randomness and diversity. Characteristics: Creative outputs, high variation, unexpected combinations, less predictable. Use for: Creative writing, brainstorming, generating ideas, diversity valued. Example: Story writing, marketing copy variations.</p><h3>Temperature 1.5+: Maximum Creativity</h3><p>Very high randomness, may be incoherent. Characteristics: Highly creative, very diverse, potentially nonsensical, unpredictable. Use for: Experimental generation, extreme brainstorming, artistic applications. Example: Abstract creative writing, unusual idea generation. Use cautiously.</p><h3>Choosing the Right Temperature</h3><p>Consider task requirements. For accuracy: Use low temperature (0-0.3). For consistency: Use low to medium (0-0.5). For creativity: Use medium to high (0.7-1.2). For diversity: Use high (1.0+). For exploration: Use very high (1.5+). Match temperature to task needs.</p><h3>Temperature and Task Type</h3><p>Different tasks need different temperatures. Factual tasks (low 0-0.3): Data extraction, classification, structured output. Analytical tasks (low-medium 0.3-0.5): Analysis, summarization, technical writing. General tasks (medium 0.5-0.8): Content creation, conversation, general writing. Creative tasks (high 0.8-1.5): Brainstorming, creative writing, idea generation. Experimental (very high 1.5+): Artistic, abstract, exploratory.</p><h3>Temperature and Quality</h3><p>Temperature affects output quality. Low temperature: Higher consistency, lower creativity, more predictable quality. High temperature: Lower consistency, higher creativity, variable quality. Find sweet spot for your use case. Test different temperatures.</p><h3>Other Creativity Parameters</h3><p>Beyond temperature, other parameters control creativity. Top-p (nucleus sampling): Limits to top probability mass. Top-k: Limits to top k words. Frequency penalty: Reduces repetition. Presence penalty: Encourages topic diversity. Combine parameters for fine control.</p><h3>Top-p (Nucleus Sampling)</h3><p>Alternative to temperature for controlling randomness. Top-p 0.1: Very focused, like low temperature. Top-p 0.5: Balanced. Top-p 0.9: Diverse, like high temperature. Top-p 1.0: All words possible. Often used with temperature for combined effect.</p><h3>Frequency and Presence Penalties</h3><p>Control repetition and topic diversity. Frequency penalty: Reduces word repetition based on frequency. Presence penalty: Reduces topic repetition. Range typically -2.0 to 2.0. Positive values discourage repetition. Use to prevent boring, repetitive outputs.</p><h3>Combining Parameters</h3><p>Use multiple parameters together. Conservative: Temperature 0.3, top-p 0.5, penalties 0. Balanced: Temperature 0.7, top-p 0.9, penalties 0.5. Creative: Temperature 1.0, top-p 0.95, penalties 1.0. Experimental: Temperature 1.5, top-p 1.0, penalties 1.5. Test combinations for your needs.</p><h3>Dynamic Temperature</h3><p>Adjust temperature based on context. Start high for brainstorming, lower for refinement. Use low for facts, high for opinions. Adapt to user preferences. Change based on task phase. Dynamic control for optimal results.</p><h3>Testing Temperature</h3><p>Experiment to find optimal temperature. Generate same prompt at different temperatures. Compare outputs for quality, creativity, consistency. Measure against success criteria. Test with representative inputs. Document what works.</p><h3>Common Temperature Mistakes</h3><p>Mistake 1: Using default without testing. Solution: Test different temperatures. Mistake 2: Too high for factual tasks. Solution: Use low temperature for accuracy. Mistake 3: Too low for creative tasks. Solution: Increase for creativity. Mistake 4: Not considering task type. Solution: Match temperature to task.</p><h3>Key Takeaways</h3><ul><li>Temperature controls randomness and creativity in AI outputs</li><li>Low (0-0.3): Consistent, factual, predictable</li><li>Medium (0.4-0.7): Balanced, general purpose</li><li>High (0.8-2.0): Creative, diverse, unpredictable</li><li>Match temperature to task requirements</li><li>Combine with top-p and penalties for fine control</li><li>Test different temperatures to find optimal</li><li>Adjust dynamically based on context</li></ul><p>In the next lesson, we will explore Zero-Shot and Self-Consistency Techniques.</p></div>'
WHERE id = '8f0ae954-d667-4c8a-b269-7d6dd78057ec';

-- LESSON 6: Zero-Shot and Self-Consistency Techniques (ID: d810a1f2-a653-4e77-bd57-eeb2c499d51f)
UPDATE lessons SET content = '<div class="lesson-content"><h2>Zero-Shot and Self-Consistency Techniques</h2><h3>Advanced Prompting Methods</h3><p>Zero-shot prompting and self-consistency are powerful techniques for improving AI performance without examples or training. This lesson teaches you these advanced methods for better results.</p><h3>What Is Zero-Shot Prompting?</h3><p>Zero-shot means no examples provided, just instructions. AI must understand and perform task from description alone. Relies on AI pre-training and general knowledge. Simpler than few-shot but can be very effective. Good for straightforward tasks.</p><h3>Effective Zero-Shot Prompts</h3><p>Make zero-shot work with clear instructions. Be specific about task and requirements. Define output format explicitly. Provide context and background. State constraints and rules. Use clear, unambiguous language. Clarity compensates for lack of examples.</p><h3>Zero-Shot vs Few-Shot</h3><p>When to use each. Zero-shot when: Task is straightforward, examples hard to create, want simplicity, context window limited. Few-shot when: Task is complex or ambiguous, format is specific, pattern not obvious, quality critical. Both have their place.</p><h3>Zero-Shot Chain-of-Thought</h3><p>Powerful zero-shot technique. Add: Let us think step by step, or Let us solve this step by step. AI shows reasoning process. Dramatically improves performance on reasoning tasks. Simple but very effective. Works across many tasks.</p><h3>What Is Self-Consistency?</h3><p>Self-consistency generates multiple solutions and chooses most common answer. Process: Generate multiple outputs (5-10), compare answers, select most frequent answer, or aggregate insights. Improves reliability through consensus.</p><h3>Why Self-Consistency Works</h3><p>Multiple attempts reduce errors. Random errors average out. Correct answer appears most often. Consensus indicates confidence. Catches mistakes through redundancy. Statistical improvement through repetition.</p><h3>Implementing Self-Consistency</h3><p>Implementation steps. Generate N outputs (typically 5-10). Use different random seeds or higher temperature. Compare outputs for consistency. Select majority answer or synthesize. Report confidence based on agreement. More agreement means higher confidence.</p><h3>Self-Consistency for Reasoning</h3><p>Particularly effective for reasoning tasks. Generate multiple reasoning paths. Each path may use different approach. Compare final answers. Choose most common answer. Verify reasoning of majority answer. Robust reasoning through diversity.</p><h3>Self-Consistency for Creative Tasks</h3><p>Also useful for creative work. Generate multiple creative options. Evaluate each option. Identify common themes or best elements. Synthesize best parts into final output. Or present multiple options to user. Diversity improves creativity.</p><h3>Voting Mechanisms</h3><p>How to choose from multiple outputs. Simple majority: Most common answer wins. Weighted voting: Weight by confidence scores. Threshold voting: Require minimum agreement. Ranked voting: Rank and select top. Choose mechanism based on task.</p><h3>Confidence Scoring</h3><p>Measure confidence in results. High agreement (80%+): High confidence. Moderate agreement (60-80%): Medium confidence. Low agreement (below 60%): Low confidence, investigate. No consensus: Task may be ambiguous or too hard. Use confidence to guide decisions.</p><h3>Combining Techniques</h3><p>Powerful combinations. Zero-shot plus self-consistency: No examples, multiple attempts. Chain-of-thought plus self-consistency: Multiple reasoning paths. Few-shot plus self-consistency: Examples with multiple attempts. Dynamic temperature plus self-consistency: Vary creativity across attempts. Combine for maximum effectiveness.</p><h3>Cost Considerations</h3><p>Self-consistency requires multiple API calls. Costs multiply by number of attempts. Balance quality improvement vs cost. Use for critical tasks where accuracy matters. May not be worth it for simple tasks. Consider cost-benefit tradeoff.</p><h3>Optimization Strategies</h3><p>Make self-consistency efficient. Start with fewer attempts (3-5). Increase if needed for confidence. Stop early if consensus reached. Use caching when possible. Parallelize API calls. Balance thoroughness and cost.</p><h3>When to Use Self-Consistency</h3><p>Best for: High-stakes decisions, reasoning and math problems, ambiguous tasks, quality critical, cost acceptable. Not necessary for: Simple tasks, creative tasks (want diversity), cost-sensitive applications, real-time requirements. Use strategically.</p><h3>Measuring Effectiveness</h3><p>Track self-consistency performance. Compare accuracy with vs without. Measure confidence calibration. Track cost vs quality improvement. Analyze disagreement patterns. Optimize based on data. Continuous improvement.</p><h3>Key Takeaways</h3><ul><li>Zero-shot prompting uses no examples, relies on clear instructions</li><li>Zero-shot chain-of-thought: add let us think step by step</li><li>Self-consistency generates multiple outputs and chooses most common</li><li>Improves reliability through consensus and redundancy</li><li>Particularly effective for reasoning tasks</li><li>Combine techniques for maximum effectiveness</li><li>Consider cost vs quality tradeoff</li><li>Use strategically for critical tasks</li></ul><p>Congratulations! You have completed Module 4. You now understand professional mastery and ethics in prompt engineering. In Module 5, we will explore real-world applications.</p></div>'
WHERE id = 'd810a1f2-a653-4e77-bd57-eeb2c499d51f';

-- MODULE 4 COMPLETE! All 6 lessons expanded!
-- Progress: 23/27 lessons complete (85.2%)
-- Next: Run PROMPT_M5_COMPLETE.sql for Module 5 (3 lessons)
